import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import seaborn as sns
from mlxtend.frequent_patterns import apriori, association_rules, fpgrowth
from sklearn.feature_extraction.text import TfidfVectorizer
import re
import jieba

# è®¾ç½®é¡µé¢æ ‡é¢˜å’Œä¸»é¢˜
st.set_page_config(page_title="æ‹›è˜æ•°æ®å¯è§†åŒ–ç³»ç»Ÿ", page_icon="ğŸ“Š", layout="wide")

# é¡µé¢æ ·å¼
st.markdown("""
    <style>
        .main {
            background-color: #f5f5f5;  /* é¡µé¢èƒŒæ™¯é¢œè‰² */
        }
        h1 {
            color: #4CAF50;
            text-align: center;
        }
        h2 {
            color: #333333;
            margin-top: 20px;
            text-align: center;
        }
        .uploadedFile {
            text-align: center;
            margin: 10px;
        }
        .stButton button {
            background-color: #4CAF50;
            color: white;
            border: None;
            padding: 10px 20px;
            border-radius: 5px;
            transition: 0.3s;
        }
        .stButton button:hover {
            background-color: #45a049;
        }
    </style>
    """, unsafe_allow_html=True)

# åº”ç”¨ç¨‹åºæ ‡é¢˜
st.title("ğŸ“Š æ‹›è˜æ•°æ®å¯è§†åŒ–ç³»ç»Ÿ")

# é¡µé¢æè¿°
st.markdown("<h2>é€šè¿‡å¯è§†åŒ–åˆ†ææå‡æ‹›è˜æ•°æ®çš„æ´å¯ŸåŠ›</h2>", unsafe_allow_html=True)

# åˆ›å»ºæ•°æ®æ¸…æ´—çš„é€‰é¡¹å¡
tab1, tab2, tab3 = st.tabs(["æ•°æ®æ¸…æ´—", "æ•°æ®åˆ†æ", "æ•°æ®å¯è§†åŒ–"])


# æ¸…æ´—å‡½æ•°ï¼šä¿ç•™æ±‰å­—ã€æ•°å­—å’Œè‹±æ–‡å­—æ¯ï¼Œå»é™¤å…¶ä»–å­—ç¬¦
def clean_text(text):
    cleaned_text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9]', ' ', text)  # æ¸…é™¤éæ±‰å­—ã€æ•°å­—å’Œè‹±æ–‡å­—æ¯çš„å­—ç¬¦
    cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()  # å»é™¤å¤šä½™çš„ç©ºæ ¼
    return cleaned_text
# å®šä¹‰æ ‡å‡†åŒ–è–ªèµ„çš„å‡½æ•°
def standardize_salary(salary):
    salary = str(salary)
    # å°† 'ä¸‡' è½¬æ¢ä¸º 'k'
    if 'ä¸‡' in salary:
        salary = re.sub(r'(\d+(\.\d+)?)ä¸‡', lambda x: str(int(float(x.group(1)) * 10)) + 'k', salary)
    # å°† 'åƒ' è½¬æ¢ä¸º 'k'
    if 'åƒ' in salary:
        salary = salary.replace('åƒ', 'k')
    return salary

# å»æ‰è–ªèµ„åˆ—ä¸­çš„ 'Â·13è–ª' ä¹‹ç±»çš„éƒ¨åˆ†
def remove_bonus(salary):
    salary = str(salary)
    salary = re.sub(r'\s*Â·\d+è–ª', '', salary)
    return salary

# è®¡ç®—å¹³å‡è–ªèµ„
def calculate_average_salary(salary):
    salary = str(salary)
    if '-' in salary:
        parts = salary.split('-')
        if len(parts) == 2:
            try:
                lower = float(parts[0].replace('k', ''))
                upper = float(parts[1].replace('k', ''))
                return f"{(lower + upper) / 2}k"
            except ValueError:
                return salary
    try:
        return f"{float(salary.replace('k', ''))}k"
    except ValueError:
        return salary

# æ•°æ®æ¸…æ´—éƒ¨åˆ†
with tab1:
    st.header("ğŸ§¹ æ•°æ®æ¸…æ´—")
    col1, col2 = st.columns(2)

    # æ•°æ®æ¸…æ´—â€”â€”æŒ–æ˜æŠ€èƒ½å…³è”è§„åˆ™
    with col1:
        st.header("æŒ–æ˜æŠ€èƒ½å…³è”è§„åˆ™")
        uploaded_file_cleaning = st.file_uploader("ä¸Šä¼ éœ€è¦æ¸…æ´—çš„CSVæ–‡ä»¶", type=["csv"], key="cleaning_uploader")

        if uploaded_file_cleaning is not None:
            df_clean = pd.read_csv(uploaded_file_cleaning)
            st.write("åŸå§‹æ•°æ®é¢„è§ˆ:", df_clean.head())
            # 1. æ•°æ®å»é‡å’Œå»é™¤éæ³•å­—ç¬¦æ•´åˆ
            if st.checkbox("ç¬¬ä¸€æ­¥ï¼šæ•°æ®å»é‡ä¸å»é™¤éæ³•å­—ç¬¦"):
                st.write("è¯·é€‰æ‹©éœ€è¦åŸºäºå“ªäº›å­—æ®µè¿›è¡Œå»é‡ï¼š")
                key_columns = st.multiselect("é€‰æ‹©å»é‡çš„å…³é”®å­—æ®µ", df_clean.columns)

                if key_columns:
                    # æä¾›æ‰§è¡ŒæŒ‰é’®
                    if st.button("æ‰§è¡Œå»é‡å’Œå»é™¤éæ³•å­—ç¬¦"):
                        # æ‰§è¡Œå»é‡
                        original_length = len(df_clean)
                        df_clean = df_clean.drop_duplicates(subset=key_columns)
                        removed_count = original_length - len(df_clean)
                        st.write(f"å»é‡å®Œæˆï¼Œå…±å»é™¤ {removed_count} æ¡é‡å¤æ•°æ®ã€‚")

                        # æ‰§è¡Œå»é™¤éæ³•å­—ç¬¦
                        if 'èŒä½æè¿°' in df_clean.columns:
                            descriptions = df_clean['èŒä½æè¿°'].dropna()
                            cleaned_descriptions = descriptions.apply(clean_text)
                            df_clean['æ¸…æ´—åçš„èŒä½æè¿°'] = cleaned_descriptions
                            st.write("æ¸…æ´—åçš„èŒä½æè¿°é¢„è§ˆ:", df_clean[['èŒä½æè¿°', 'æ¸…æ´—åçš„èŒä½æè¿°']].head())

                            # å°†æ‰€æœ‰çš„æŠ€èƒ½æè¿°å†™å…¥åˆ°TXTæ–‡ä»¶
                            skills_list = cleaned_descriptions.tolist()  # è½¬ä¸ºåˆ—è¡¨
                            output_file = "æŠ€èƒ½æè¿°.txt"  # å®šä¹‰è¾“å‡ºæ–‡ä»¶å

                            # å°†æŠ€èƒ½æè¿°å†™å…¥å†…å­˜ä¸­çš„å­—ç¬¦ä¸²
                            skills_text = "\n".join([f"[{skill}]" for skill in skills_list])

                            # æä¾›ä¸‹è½½æ¸…æ´—åçš„æ•°æ®
                            # cleaned_file_after = df_clean.to_csv(index=False).encode('utf-8')
                            # st.download_button("ä¸‹è½½æ¸…æ´—åçš„æ•°æ®", cleaned_file_after, "cleaned_data_after.csv",
                            #                    "text/csv")

                            # æä¾›ä¸‹è½½æŠ€èƒ½æè¿°çš„TXTæ–‡ä»¶
                            st.download_button("ä¸‹è½½æŠ€èƒ½æè¿°", skills_text, "æŠ€èƒ½æè¿°.txt", "text/plain")

                        else:
                            st.warning("æ•°æ®ä¸­ä¸åŒ…å«'èŒä½æè¿°'åˆ—ã€‚")
                else:
                    st.warning("è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªå­—æ®µè¿›è¡Œå»é‡ã€‚")

            # 2. ç»Ÿä¸€æŠ€èƒ½åç§°
            if st.checkbox("ç¬¬äºŒæ­¥ï¼šç»Ÿä¸€èŒä½æè¿°çš„æŠ€èƒ½åç§°å¤§å°å†™"):
                # å…è®¸ç”¨æˆ·ä¸Šä¼ ç¬¬ä¸€æ­¥æ¸…æ´—åçš„TXTæ–‡ä»¶
                uploaded_file_previous_cleaned_txt = st.file_uploader("ä¸Šä¼ å»é‡åçš„TXTæ–‡ä»¶", type=["txt"],
                                                                      key="previous_cleaned_txt_uploader")

                if uploaded_file_previous_cleaned_txt is not None:
                    # è¯»å–TXTæ–‡ä»¶å†…å®¹
                    skills_list = uploaded_file_previous_cleaned_txt.read().decode('utf-8').strip().splitlines()

                    # ç»Ÿä¸€æŠ€èƒ½åç§°ä¸ºå°å†™
                    unified_skills = [skill.lower() for skill in skills_list]

                    # æ˜¾ç¤ºç»Ÿä¸€åçš„æŠ€èƒ½åç§°é¢„è§ˆ
                    st.write("ç»Ÿä¸€åçš„èŒä½æè¿°é¢„è§ˆ:", unified_skills[:10])  # åªæ˜¾ç¤ºå‰10ä¸ª

                    # å°†ç»Ÿä¸€åçš„æŠ€èƒ½åç§°ä¿å­˜ä¸ºTXTæ–‡ä»¶
                    unified_skills_text = "\n".join([f"[{skill}]" for skill in unified_skills])

                    # æä¾›ä¸‹è½½ç»Ÿä¸€åçš„æ•°æ®
                    st.download_button("ä¸‹è½½ç»Ÿä¸€åçš„æŠ€èƒ½æè¿°", unified_skills_text, "unified_skills.txt", "text/plain")

            # 3. tdf-idfè¯é¢‘æå–
            if st.checkbox("ç¬¬ä¸‰æ­¥ï¼šè®¡ç®—è¯é¢‘å¹¶æ˜¾ç¤ºå‰80ä¸ªè¯"):
                uploaded_file_previous_cleaned_txt = st.file_uploader("ä¸Šä¼ ç»Ÿä¸€æŠ€èƒ½åç§°çš„TXTæ–‡ä»¶", type=["txt"],
                                                                      key="previous_cleaned_txt_uploader2")

                if uploaded_file_previous_cleaned_txt is not None:
                    # è¯»å–TXTæ–‡ä»¶å†…å®¹
                    skills_list = uploaded_file_previous_cleaned_txt.read().decode('utf-8').strip().splitlines()

                    # ä½¿ç”¨æ¯è¡Œçš„æŠ€èƒ½è¯ä½œä¸ºè¾“å…¥è¿›è¡ŒTF-IDFå¤„ç†
                    # å‡è®¾æ¯è¡Œéƒ½æ˜¯ä¸€ä¸ªæŠ€èƒ½æè¿°
                    vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 2))
                    X = vectorizer.fit_transform(skills_list)

                    # æå–ç‰¹å¾åç§°ï¼ˆè¯æ±‡ï¼‰
                    feature_names = vectorizer.get_feature_names_out()

                    # åˆ›å»ºä¸€ä¸ª DataFrameï¼Œå±•ç¤ºæ¯ä¸ªè¯æ±‡çš„ TF-IDF åˆ†æ•°
                    df_tfidf = pd.DataFrame(X.T.toarray(), index=feature_names)


                    # è¿‡æ»¤æ‰åŒ…å«ä¸­æ–‡å­—ç¬¦çš„è¯æ±‡
                    def contains_chinese(text):
                        return bool(re.search(r'[\u4e00-\u9fff]', text))


                    # åªä¿ç•™ä¸åŒ…å«ä¸­æ–‡å­—ç¬¦çš„è¯æ±‡
                    df_tfidf = df_tfidf[~df_tfidf.index.map(contains_chinese)]

                    # è®¡ç®—æ¯ä¸ªè¯æ±‡çš„å¹³å‡ TF-IDF åˆ†æ•°
                    df_tfidf['mean_score'] = df_tfidf.mean(axis=1)

                    # æŒ‰ç…§å¹³å‡ TF-IDF åˆ†æ•°é™åºæ’åº
                    df_tfidf = df_tfidf.sort_values(by='mean_score', ascending=False)

                    # æ˜¾ç¤ºå‰80ä¸ªæœ€é‡è¦çš„è¯æ±‡
                    st.write("TF-IDF åˆ†æ•°æœ€é«˜çš„å‰ 80 ä¸ªè¯æ±‡ï¼š")
                    st.dataframe(df_tfidf[['mean_score']].head(80))  # åªæ˜¾ç¤ºå‰80è¡Œ
                    st.write("è¯·è‡ªè¡Œæ•´ç†å‡ºæ’åé å‰çš„æŠ€èƒ½è¯æ±‡ï¼Œä¸ºä¸‹ä¸€æ­¥åˆ†è¯åšå‡†å¤‡")

                    # 4. åˆ†è¯å¤„ç†
            if st.checkbox("ç¬¬å››æ­¥ï¼šæå–æŠ€èƒ½è¯æ±‡å¹¶ä½¿ç”¨è‡ªå®šä¹‰åˆ†è¯"):
                # ç”¨æˆ·è‡ªå®šä¹‰æŠ€èƒ½è¯æ±‡è¡¨
                custom_skills_input = st.text_area(
                    "è¯·è¾“å…¥è‡ªå®šä¹‰æŠ€èƒ½è¯æ±‡ï¼ˆä»¥é€—å·åˆ†éš”ï¼‰:",
                    value=""
                )

                # å¤„ç†è‡ªå®šä¹‰æŠ€èƒ½è¯æ±‡
                custom_skills = [skill.strip().lower() for skill in custom_skills_input.split(",")]

                # å°†æŠ€èƒ½è¯æ±‡åŠ å…¥åˆ° jieba åˆ†è¯è¯å…¸ä¸­
                for skill in custom_skills:
                    jieba.add_word(skill)


                # å®šä¹‰åˆ†è¯å¹¶æå–æŠ€èƒ½è¯æ±‡çš„å‡½æ•°
                def extract_skills(text):
                    # å¤„ç†ç»„åˆè¯
                    text = re.sub(r'spring\s+boot', 'springboot', text, flags=re.IGNORECASE)
                    text = re.sub(r'spring\s+mvc', 'springmvc', text, flags=re.IGNORECASE)
                    text = re.sub(r'spring\s+cloud', 'springcloud', text, flags=re.IGNORECASE)
                    text = re.sub(r'\bjs\b', 'JavaScript', text, flags=re.IGNORECASE)

                    # ä½¿ç”¨ jieba åˆ†è¯
                    words = jieba.lcut(text)
                    extracted_skills = [word.lower() for word in words if word.lower() in custom_skills]

                    # å»é™¤é‡å¤è¯æ±‡
                    unique_skills = list(set(extracted_skills))

                    return unique_skills


                # ä¸Šä¼ æ–‡ä»¶
                uploaded_file = st.file_uploader("ä¸Šä¼ æ–‡æœ¬æ–‡ä»¶", type="txt")

                if uploaded_file is not None:
                    content = uploaded_file.read().decode("utf-8").splitlines()
                    all_extracted_skills = []

                    for line in content:
                        skills_extracted = extract_skills(line)
                        all_extracted_skills.append(skills_extracted)

                    # å°†æå–çš„æŠ€èƒ½å†™å…¥æ–°çš„æ–‡æœ¬æ–‡ä»¶
                    output_file_content = "\n".join(
                        [f"[{', '.join(skills)}]" for skills in all_extracted_skills if skills])

                    # æä¾›ä¸‹è½½é“¾æ¥
                    st.download_button(
                        label="ä¸‹è½½æå–çš„æŠ€èƒ½è¯æ±‡",
                        data=output_file_content,
                        file_name="extracted_skills.txt",
                        mime="text/plain"
                    )
                    st.success("è‡³æ­¤æ¸…æ´—æ­¥éª¤ç»“æŸï¼Œè¯·ç§»è‡³æ•°æ®åˆ†ææ¨¡å—")




















        else:
            st.info("è¯·ä¸Šä¼ éœ€è¦æ¸…æ´—çš„CSVæ–‡ä»¶ã€‚")



    # æ•°æ®æ¸…æ´—â€”â€”å¯è§†åŒ–å­¦å†ï¼ŒåŸå¸‚ï¼Œè–ªèµ„çš„å…³ç³»
    with col2:
        st.header("å¯è§†åŒ–å­¦å†ã€åŸå¸‚ä¸è–ªèµ„çš„å…³ç³»")
        uploaded_file = st.file_uploader("ä¸Šä¼ åŒ…å«è–ªèµ„æ•°æ®çš„CSVæ–‡ä»¶", type=["csv"])

        if uploaded_file is not None:
            # è¯»å– CSV æ–‡ä»¶
            df = pd.read_csv(uploaded_file)

            if 'è–ªèµ„' in df.columns:
                # æ ‡å‡†åŒ–è–ªèµ„åˆ—
                df['è–ªèµ„'] = df['è–ªèµ„'].apply(standardize_salary)
                df['è–ªèµ„'] = df['è–ªèµ„'].apply(remove_bonus)
                df['è–ªèµ„'] = df['è–ªèµ„'].apply(calculate_average_salary)

                # åˆ é™¤éæ ‡å‡†åŒ–è–ªèµ„ï¼ˆä¸åŒ…å« 'k' çš„è–ªèµ„ï¼‰
                df = df[df['è–ªèµ„'].str.contains(r'\d+k$', na=False)]

                # æ˜¾ç¤ºå¤„ç†åçš„æ•°æ®
                st.write("å¤„ç†åçš„è–ªèµ„æ•°æ®é¢„è§ˆï¼š")
                st.dataframe(df)

                # è½¬æ¢ä¸º CSV æ ¼å¼
                csv_data = df.to_csv(index=False).encode('utf-8')

                # ä¸‹è½½æŒ‰é’®ï¼Œå…è®¸ç”¨æˆ·ä¸‹è½½å¤„ç†åçš„ CSV æ–‡ä»¶
                st.download_button(
                    label="ä¸‹è½½å¤„ç†åçš„æ•°æ®",
                    data=csv_data,
                    file_name="processed_salary_data.csv",
                    mime="text/csv"
                )
            else:
                st.warning("CSV æ–‡ä»¶ä¸­æœªæ‰¾åˆ° 'è–ªèµ„' åˆ—ã€‚")
        else:
            st.info("è¯·ä¸Šä¼ åŒ…å«è–ªèµ„æ•°æ®çš„CSVæ–‡ä»¶ã€‚")

# æ•°æ®åˆ†æéƒ¨åˆ†
import streamlit as st
import pandas as pd
import re
from mlxtend.frequent_patterns import apriori, association_rules, fpgrowth

# åˆ›å»º tab2
with tab2:
    # è®¾ç½®é¡µé¢æ ‡é¢˜
    st.header(" å…³è”è§„åˆ™åˆ†æ")

    # æ–‡ä»¶ä¸Šä¼ 
    uploaded_file = st.file_uploader("ä¸Šä¼ åŒ…å«æŠ€èƒ½è¯æ±‡çš„ TXT æ–‡ä»¶", type=["txt"])

    # æ”¯æŒåº¦å’Œç½®ä¿¡åº¦æ»‘åŠ¨æ¡
    min_support = st.slider("é€‰æ‹©æœ€å°æ”¯æŒåº¦ï¼ˆSupportï¼‰", 0.01, 1.0, 0.2, step=0.01)
    min_confidence = st.slider("é€‰æ‹©æœ€å°ç½®ä¿¡åº¦ï¼ˆConfidenceï¼‰", 0.01, 1.0, 0.75, step=0.01)

    # é€‰æ‹©ç®—æ³•ï¼šApriori æˆ– FP-Growth
    algorithm = st.selectbox("é€‰æ‹©å…³è”è§„åˆ™æŒ–æ˜ç®—æ³•", ["Apriori", "FP-Growth"])

    if uploaded_file is not None:
        # è¯»å–æ–‡ä»¶å†…å®¹
        lines = uploaded_file.read().decode('utf-8').splitlines()

        # è‡ªå®šä¹‰è§£ææ¯ä¸€è¡Œçš„æŠ€èƒ½åˆ—è¡¨
        def parse_line(line):
            cleaned_line = line.strip().strip('[]')
            skills = [skill.strip() for skill in cleaned_line.split(',')]
            return skills

        # ä½¿ç”¨è‡ªå®šä¹‰è§£æå‡½æ•°å¤„ç†æ¯ä¸€è¡Œ
        transactions = [parse_line(line) for line in lines]

        # è½¬æ¢ä¸º DataFrameï¼Œåˆ›å»º one-hot ç¼–ç 
        all_skills = sorted(set(skill for sublist in transactions for skill in sublist))
        onehot_encoded = [{skill: (skill in transaction) for skill in all_skills} for transaction in transactions]
        df = pd.DataFrame(onehot_encoded)

        # æ˜¾ç¤º One-Hot ç¼–ç çš„æ•°æ®é¢„è§ˆ
        st.write("One-Hot ç¼–ç åçš„æ•°æ®ï¼š")
        st.dataframe(df.head())

        # æ ¹æ®ç”¨æˆ·é€‰æ‹©çš„ç®—æ³•è¿è¡Œåˆ†æ
        if st.button("è¿è¡Œç®—æ³•åˆ†æ"):
            with st.spinner(f'æ­£åœ¨è®¡ç®—é¢‘ç¹é¡¹é›†å’Œå…³è”è§„åˆ™ ({algorithm})...'):
                # æ ¹æ®é€‰æ‹©çš„ç®—æ³•è®¡ç®—é¢‘ç¹é¡¹é›†
                if algorithm == "Apriori":
                    frequent_itemsets = apriori(df, min_support=min_support, use_colnames=True)
                else:
                    frequent_itemsets = fpgrowth(df, min_support=min_support, use_colnames=True)

                st.write(f"é¢‘ç¹é¡¹é›† ({algorithm})ï¼š")
                st.dataframe(frequent_itemsets)

                # ç”Ÿæˆå…³è”è§„åˆ™
                rules = association_rules(frequent_itemsets, metric="confidence", min_threshold=min_confidence)

                # åªä¿ç•™ä¸‰é¡¹åŠä»¥ä¸Šçš„è§„åˆ™
                three_or_more_item_rules = rules[rules['antecedents'].apply(lambda x: len(x) > 2)]

                # é¿å… SettingWithCopyWarning
                three_or_more_item_rules = three_or_more_item_rules.copy()
                three_or_more_item_rules.loc[:, 'antecedents'] = three_or_more_item_rules['antecedents'].apply(
                    lambda x: ', '.join(x))
                three_or_more_item_rules.loc[:, 'consequents'] = three_or_more_item_rules['consequents'].apply(
                    lambda x: ', '.join(x))

                st.write(f"ä¸‰é¡¹åŠä»¥ä¸Šçš„å…³è”è§„åˆ™ ({algorithm})ï¼š")
                st.dataframe(three_or_more_item_rules)

                # ä¸‹è½½æŒ‰é’®ï¼šé¢‘ç¹é¡¹é›†
                frequent_itemsets_csv = frequent_itemsets.to_csv(index=False).encode('utf-8')
                st.download_button(
                    label=f"ä¸‹è½½é¢‘ç¹é¡¹é›† ({algorithm}) CSV",
                    data=frequent_itemsets_csv,
                    file_name=f"frequent_itemsets_{algorithm.lower()}.csv",
                    mime="text/csv"
                )

                # ä¸‹è½½æŒ‰é’®ï¼šå…³è”è§„åˆ™
                rules_csv = three_or_more_item_rules.to_csv(index=False).encode('utf-8')
                st.download_button(
                    label=f"ä¸‹è½½å…³è”è§„åˆ™ ({algorithm}) CSV",
                    data=rules_csv,
                    file_name=f"association_rules_{algorithm.lower()}.csv",
                    mime="text/csv"
                )
    else:
        st.info("è¯·ä¸Šä¼ åŒ…å«æŠ€èƒ½è¯æ±‡çš„ TXT æ–‡ä»¶ã€‚")


# æ•°æ®å¯è§†åŒ–éƒ¨åˆ†
with tab3:
    # ç¬¬ä¸€è¡Œ: è¯äº‘å’Œçƒ­åŠ›å›¾
    col1, col2 = st.columns(2)

    # ç¬¬ä¸€åˆ— - è¯äº‘å›¾å±•ç¤º
    with col1:
        st.header("ğŸ” è¯äº‘å›¾å±•ç¤º")
        uploaded_file_wordcloud = st.file_uploader("ä¸Šä¼ è¯äº‘å›¾CSVæ–‡ä»¶", type=["csv"], key="wordcloud_uploader_1")
        if uploaded_file_wordcloud is not None:
            with st.spinner('æ­£åœ¨ç”Ÿæˆè¯äº‘å›¾...'):
                df_wordcloud = pd.read_csv(uploaded_file_wordcloud)
                skills = []
                for index, row in df_wordcloud.iterrows():
                    skill_set = eval(row['itemsets'])  # å‡è®¾ 'itemsets' åˆ—ä¸º frozen set
                    skills.extend(skill_set)
                skill_freq = pd.Series(skills).value_counts()

                # åˆ›å»ºè¯äº‘
                wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(
                    skill_freq)
                plt.figure(figsize=(10, 5))
                plt.imshow(wordcloud, interpolation='bilinear')
                plt.axis('off')
                st.pyplot(plt)
        else:
            st.info("è¯·ä¸Šä¼ CSVæ–‡ä»¶ä»¥ç”Ÿæˆè¯äº‘å›¾ã€‚")

    # ç¬¬äºŒåˆ— - çƒ­åŠ›å›¾å±•ç¤º
    with col2:
        st.header("ğŸ”¥ å…³è”è§„åˆ™çƒ­åŠ›å›¾å±•ç¤º")
        uploaded_file_heatmap = st.file_uploader("ä¸Šä¼ çƒ­åŠ›å›¾CSVæ–‡ä»¶", type=["csv"], key="heatmap_uploader_1")
        if uploaded_file_heatmap is not None:
            with st.spinner('æ­£åœ¨ç”Ÿæˆçƒ­åŠ›å›¾...'):
                df_heatmap = pd.read_csv(uploaded_file_heatmap)
                heatmap_data = df_heatmap.pivot_table(index='antecedents', columns='consequents', values='confidence',
                                                      fill_value=0)

                plt.figure(figsize=(20, 12))
                sns.heatmap(heatmap_data, annot=True, fmt=".2f", cmap='YlGnBu', cbar_kws={'label': 'Confidence'})
                plt.title('å…³è”è§„åˆ™çƒ­åŠ›å›¾', fontsize=20)
                st.pyplot(plt)
        else:
            st.info("è¯·ä¸Šä¼ CSVæ–‡ä»¶ä»¥ç”Ÿæˆçƒ­åŠ›å›¾ã€‚")

    # ç¬¬äºŒè¡Œ: åŸå¸‚å·¥èµ„æŸ±å½¢å›¾å’Œå­¦å†è–ªèµ„æŠ˜çº¿å›¾
    col3, col4 = st.columns(2)

    # åŸå¸‚å·¥èµ„æŸ±å½¢å›¾å±•ç¤º
    with col3:
        plt.rcParams['font.sans-serif'] = ['SimHei']
        plt.rcParams['axes.unicode_minus'] = False
        st.header("ğŸ™ï¸ å„åŸå¸‚å·¥èµ„æŸ±å½¢å›¾")
        uploaded_file_salary = st.file_uploader("ä¸Šä¼ åŸå¸‚å·¥èµ„æ•°æ®CSVæ–‡ä»¶", type=["csv"], key="salary_uploader_1")
        if uploaded_file_salary is not None:
            with st.spinner('æ­£åœ¨ç”ŸæˆåŸå¸‚å·¥èµ„æŸ±å½¢å›¾...'):
                salary_data = pd.read_csv(uploaded_file_salary)
                salary_data['City'] = salary_data['æ‰€åœ¨åŸå¸‚'].str[:2]
                salary_data['Salary'] = salary_data['è–ªèµ„'].str.extract(r'(\d+\.?\d*)').astype(float)
                city_salary_mean = salary_data.groupby('City')['Salary'].mean().reset_index()

                plt.figure(figsize=(10, 6))
                plt.bar(city_salary_mean['City'], city_salary_mean['Salary'], color='blue', alpha=0.7)
                plt.title('å„åŸå¸‚å¹³å‡å·¥èµ„å¯¹æ¯”', fontsize=20)
                plt.xlabel('åŸå¸‚', fontsize=14)
                plt.ylabel('å¹³å‡å·¥èµ„ (K)', fontsize=14)
                plt.xticks(rotation=45)
                st.pyplot(plt)
        else:
            st.info("è¯·ä¸Šä¼ CSVæ–‡ä»¶ä»¥ç”ŸæˆåŸå¸‚å·¥èµ„æŸ±å½¢å›¾ã€‚")

    # å­¦å†è–ªèµ„æŠ˜çº¿å›¾å±•ç¤º
    with col4:
        st.header("ğŸ“ˆ å­¦å†ä¸è–ªèµ„å˜åŒ–æŠ˜çº¿å›¾")
        uploaded_file_line = st.file_uploader("ä¸Šä¼ æ•°æ®CSVæ–‡ä»¶", type=["csv"], key="line_uploader_1")

        if uploaded_file_line is not None:
            with st.spinner('æ­£åœ¨ç”ŸæˆæŠ˜çº¿å›¾...'):
                df_line = pd.read_csv(uploaded_file_line)
                df_line['è–ªèµ„'] = df_line['è–ªèµ„'].str.replace('k', '').astype(float)
                education_order = {'å¤§ä¸“': 1, 'æœ¬ç§‘': 2, 'ç¡•å£«': 3, 'åšå£«': 4}
                df_line['å­¦å†é¡ºåº'] = df_line['å­¦å†'].map(education_order)
                df_grouped = df_line.groupby('å­¦å†é¡ºåº')['è–ªèµ„'].mean().reset_index()
                df_grouped['å­¦å†'] = df_grouped['å­¦å†é¡ºåº'].map({v: k for k, v in education_order.items()})

                plt.figure(figsize=(10, 6))
                plt.plot(df_grouped['å­¦å†'], df_grouped['è–ªèµ„'], marker='o', color='orange')
                plt.title('å­¦å†ä¸å¹³å‡è–ªèµ„å˜åŒ–', fontsize=20)
                plt.xlabel('å­¦å†', fontsize=14)
                plt.ylabel('å¹³å‡è–ªèµ„ (K)', fontsize=14)
                plt.grid(True)
                st.pyplot(plt)
        else:
            st.info("è¯·ä¸Šä¼ CSVæ–‡ä»¶ä»¥ç”ŸæˆæŠ˜çº¿å›¾ã€‚")

# åº•éƒ¨ç‰ˆæƒä¿¡æ¯
st.markdown("Â© 2024 æ‹›è˜æ•°æ®å¯è§†åŒ–ç³»ç»Ÿ - All rights reserved.", unsafe_allow_html=True)
